AzureDatabricks-session12==============================
dbutils.fs.mount(source='wasbs://inputdatasets@ttstorageaccount100.blob.core.windows.net',
mount_point='/mnt/retaildb',
extra_configs={'fs.azure.account.key.ttstorageaccount100.blob.core.windows.net':'SuTUyxyYr/ooc0gFAbqwq3cRt5ApE3sAbCMBSLL8trA/jBt0BEiPRYliwgCCPDSBNY4zvC1eHLBj+AStdIk73A=='})

3 ways to access the storage account
======================================
1.accesskey/accountkey-all access to the storageaccount
2.SAS key-Shared access signature-(we can give access at container also)
3.Service principal-fine grained control on folder level

storageaccount->multiplecontainers->inside each container there can be multiple directories/folders 

dbutils.fs.mount(source='wasbs://retaildb@ttstorageaccount102.blob.core.windows.net',
mount_point='/mnt/retaildb',
extra_configs={'fs.azure.account.key.ttstorageaccount102.blob.core.windows.net':'WLRLRZvhDf0FtiEr75hG5CWQYZXzhhs573DxUGBJPvSxWutOxo+WG0mVQxBp+QI+bVkhYBSaDh5X+ASt+wLEMw=='})

df1=spark.read.csv('/mnt/retaildb/raw/customers.csv',header=True)
display(df1)

SecretScope
=============
1.azure key vault backed \secretscope-recommended any other azure service canals or euse the key create an azure key vault secrets databricks demo key

#secrets/create Scope secret scope->store many secret keys databricks-demo-scope in databricks we create da secret scope the key sare stored in azure key vault databricks secretslist -scopes

Scope Backend Key VaultURL
------------------------------------------------------------------------------
databricks-demo-scope AZURE_KEYVAULT https://ttdatabricksdemokv.vault.azure.net/
dbutils.fs.mount(source='wasbs://retaildb@ttstorageaccount102.blob.core.windows.net',
mount_point='/mnt/retaildb1',
extra_configs={'fs.azure.account.key.ttstorageaccount102.blob.core.windows.net':dbutils.secrets.get('databricks-demo-scope','databricksnewkey')})

df1=spark.read.csv('/mnt/retaildb1/raw/customers.csv',header=True)

display(df1)

dbutils.fs.mount(source='wasbs://retaildb@ttstorageaccount102.blob.core.windows.net',
mount_point='/mnt/retaildb1',
extra_configs={'fs.azure.account.key.ttstorageaccount102.blob.core.windows.net':dbutils.secrets.get('databricks-demo-scope','databricksnewkey')})

databrickssecretslist--scopedatabricks-demo-scope

Key name Last updated
-------------------------------
databricks demo key 1660770175000databricksnewkey16607710400002.databricksbackedsecretscope-encrytpeddatabricksdatabaseAzureDatabricks-

session13
==============================
Databricks backed secret scope 
CLI,or API, but we cannot do using UI
databricks secrets create-scope--scopedempscopedatabricks backed--initial-manage-principalusersdatabrickssecretslist-scopes

Scope Back end Key Vault URL
----------------------------------------------------------------------------------
databricks-demo-scopeAZURE_KEYVAULThttps://ttdatabricksdemokv.vault.azure.net/dempscopedatabricksbackedDATABRICKSN/Adatabrickssecretsput--scopedempscopedatabricksbacked--keystorageKey
databrickssecretslist--scopedempscopedatabricksbackedKeynameLastupdated
------------------------
storageKey1660772322857

dbutils.fs.mount(source='wasbs://retaildb@ttstorageaccount102.blob.core.windows.net',mount_point='/mnt/retaildb2',extra_configs={'fs.azure.account.key.ttstorageaccount102.blob.core.windows.net':dbutils.secrets.get('dempscopedatabricksbacked','storageKey')})


df1=spark.read.csv('/mnt/retaildb2/raw/customers.csv',header=True)

AzureDatabricks-session14
==============================
3waystoaccessthestorageaccount
======================================

1.accesskey/accountkey-allaccesstothestorageaccount
2.SASkey-Sharedaccesssignature-(wecangiveaccessatcontaineralso)
3.Serviceprincipal-finegrainedcontrolonfolderlevelSASKey========sv=2021-06-08&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2022-08-18T06:01:06Z&st=2022-08-17T22:01:06Z&spr=https&sig=yoe%2FgjGkmmhwDJkqbpQHJaDmfEtDjGulKPgMzkBRUuQ%3Dspark.conf.set("fs.azure.account.auth.type.ttstorageaccount102.dfs.core.windows.net","SAS")spark.conf.set("fs.azure.sas.token.provider.type.ttstorageaccount102.dfs.core.windows.net","org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider")spark.conf.set("fs.azure.sas.fixed.token.ttstorageaccount102.dfs.core.windows.net","sv=2021-06-08&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2022-08-18T06:01:06Z&st=2022-08-17T22:01:06Z&spr=https&sig=yoe%2FgjGkmmhwDJkqbpQHJaDmfEtDjGulKPgMzkBRUuQ%3D")df=spark.read.csv("abfs://retaildb@ttstorageaccount102.dfs.core.windows.net/raw/orders.csv",header=True)display(df)ServicePrincipal===================azureactivedirectory->appregistrationsdatabricks-demo-spnew
Application(client)ID9f178d90-1bb3-43dc-b60f-51552e64e418ObjectIDb258c88f-091a-43f2-9314-9d2cd4c9f873Directory(tenant)IDa216c39d-a41f-4225-8bca-11c3f51bb637gotocertificatesandsecretspQV8Q~VXv9CLGePn.ER3M51tMb-lNsygcG-fVbrCserviceprincipaliscreatedbutweneedtogiveaccesstoourstorageaccountclickonaccesscontrolIAMspark.conf.set("fs.azure.account.auth.type.ttstorageaccount102.dfs.core.windows.net","OAuth")spark.conf.set("fs.azure.account.oauth.provider.type.ttstorageaccount102.dfs.core.windows.net","org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")spark.conf.set("fs.azure.account.oauth2.client.id.ttstorageaccount102.dfs.core.windows.net","9f178d90-1bb3-43dc-b60f-51552e64e418")spark.conf.set("fs.azure.account.oauth2.client.secret.ttstorageaccount102.dfs.core.windows.net","pQV8Q~VXv9CLGePn.ER3M51tMb-lNsygcG-fVbrC")spark.conf.set("fs.azure.account.oauth2.client.endpoint.ttstorageaccount102.dfs.core.windows.net","https://login.microsoftonline.com/a216c39d-a41f-4225-8bca-11c3f51bb637/oauth2/token")df=spark.read.csv("abfs://retaildb@ttstorageaccount102.dfs.core.windows.net/raw/orders.csv",header
=True)display(df)